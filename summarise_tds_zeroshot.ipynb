{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeroshot Prompting for Blurb Generation  \n",
    "\n",
    "\n",
    "By providing a single zeroshot prompt, use llama3 or OpenAI LLMs to produce a blurb for a collection of articles from Towards Data Science.\n",
    "\n",
    "Inputs:\n",
    "- Articles in .txt format in the relative folder destination `./tds`\n",
    "- Choice of LLM `params['model']`\n",
    "- Hyperparameters in the variables `params['model']` and `params['temperature']`\n",
    "- OpenAI API key set as the variable `os.environ['OPENAI_API_KEY']`\n",
    "\n",
    "Outputs:\n",
    "- Log file saved in the home directory as `./zeroshot_summarise_ubs_week_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"[key]\"\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation Hyperparameters  \n",
    "\n",
    "Adjust the following variables to match your preferences:\n",
    "- Articles in .txt format in the relative folder destination `./tds`\n",
    "- Choice of LLM `params['model']`\n",
    "- Hyperparameters in the variables `params['model']` and `params['temperature']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./zeroshot_blurb.csv\"\n",
    "params = {}\n",
    "\n",
    "doc_folder = 'tds'\n",
    "params['doc_names'] = [os.path.join(doc_folder,f) for f in os.listdir(doc_folder) if os.path.isfile(os.path.join(doc_folder,f))]\n",
    "params['doc_names'].sort()\n",
    "params['latest_article_date'] = params['doc_names'][-1][:10]\n",
    "\n",
    "# Latest models available as of 25 June 2024\n",
    "#params['model'] = \"llama2:70b\"\n",
    "#params['model'] = \"llama3\"\n",
    "params['model'] = \"gpt-3.5-turbo-0125\"\n",
    "params['model'] = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "params['temperature'] = 0\n",
    "\n",
    "if 'llama3' in params['model']:\n",
    "    llm = Ollama(model=params['model'], num_ctx = 4096,num_predict=-1, num_gpu=1, temperature=params['temperature'],\n",
    "                    stop=[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\", \"<|reserved_special_token\"])\n",
    "elif 'llama2' in params['model']:\n",
    "    llm = Ollama(model=params['model'], num_ctx = 4096, num_predict=-1, num_gpu=1, temperature=params['temperature'])   # ,num_ctx=2048\n",
    "else:\n",
    "    llm = ChatOpenAI(temperature=0, model_name=params['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts  \n",
    "Setup your prompt templates:\n",
    "- System prompt\n",
    "- Summarisation template to write a blurb for a magazine article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in params['model']:\n",
    "    # System Prompt\n",
    "    params['system'] = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a an experienced writer with expertise in presenting technical information to a general audience in a tone that is educational, informative, and approachable.<|eot_id|>\"\n",
    "    # User Prompt\n",
    "    params['user'] = \"<|start_header_id|>user<|end_header_id|>The following is a magazine article:\\n{docs}\\n Based on this article, write a paragraph under 100 words that will convince readers to read the full article. Start the paragraph with a key reason, following up with key points and a brief context that helps to situate the article within broader trends or ongoing discussions. \\nHelpful Answer:<|eot_id|>\"\n",
    "else:\n",
    "    params['system'] = \"You are a an experienced writer with expertise in presenting technical information to a general audience in a tone that is educational, informative, and approachable.<|eot_id|>\"\n",
    "    params['user'] = \"The following is a magazine article:\\n{docs}\\n Based on this article, write a paragraph under 100 words that will convince readers to read the full article. Start the paragraph with a key reason, following up with key points and a brief context that helps to situate the article within broader trends or ongoing discussions. \\nHelpful Answer:\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", params['system']),(\"user\", params['user'])])\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Magazine Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_texts = []\n",
    "for doc in params['doc_names']:\n",
    "    with open(doc, encoding=\"utf-8\") as fd:\n",
    "        doc_texts.append(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Blurbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['blurbs'] = []\n",
    "\n",
    "for doc_text in doc_texts:\n",
    "    output = chain.invoke(doc_text)\n",
    "    params['blurbs'].append(output['text'])\n",
    "    print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Log File  \n",
    "Write all input hyperparameters and LLM outputs into a log file saved in the home directory as `./zeroshot_summarise_ubs_week_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "params['experiment_date'] = now.strftime(\"%Y-%m-%d\")\n",
    "params['experiment_time'] = now.strftime(\"%H:%M:%S\")\n",
    "try:  \n",
    "    all_params_df = pd.read_csv(log_file)\n",
    "except:\n",
    "    all_params_df = pd.DataFrame()\n",
    "\n",
    "params_df = pd.DataFrame([params])\n",
    "all_params_df = pd.concat([all_params_df,params_df])\n",
    "\n",
    "all_params_df.to_csv(log_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
