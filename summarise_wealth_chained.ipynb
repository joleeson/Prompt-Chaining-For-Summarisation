{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chained Prompting for Summary Generation  \n",
    "\n",
    "\n",
    "By providing a a sequence of chained promopts, use llama3 or OpenAI LLMs to summarise a collection of articles on wealth management news and advice.\n",
    "\n",
    "Inputs:\n",
    "- Articles in .txt format in the relative folder destination `./insight_docs`\n",
    "- Choice of LLM `params['model']`\n",
    "- Hyperparameters in the variables `params['model']` and `params['temperature']`\n",
    "- OpenAI API key set as the variable `os.environ['OPENAI_API_KEY']`\n",
    "\n",
    "Outputs:\n",
    "- Log file saved in the home directory as `./zeroshot_summarise_ubs_week_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"[key]\"\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation Hyperparameters  \n",
    "\n",
    "Adjust the following variables to match your preferences:\n",
    "- Articles in .txt format in the relative folder destination `./insight_docs`\n",
    "- Choice of LLM `params['model']`\n",
    "- Hyperparameters in the variables `params['model']` and `params['temperature']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./custom_summarise_ubs_week_log.csv\"\n",
    "params = {}\n",
    "\n",
    "doc_folder = 'insights_docs'\n",
    "params['doc_names'] = [os.path.join(doc_folder,f) for f in os.listdir(doc_folder) if os.path.isfile(os.path.join(doc_folder,f))]\n",
    "params['doc_names'].sort()\n",
    "params['latest_article_date'] = params['doc_names'][-1][:10]\n",
    "\n",
    "# Latest models available as of 25 June 2024\n",
    "#params['model'] = \"llama2:70b\"\n",
    "#params['model'] = \"llama3\"\n",
    "params['model'] = \"gpt-3.5-turbo-0125\"\n",
    "#params['model'] = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "params['reduce_token_max'] = 4000\n",
    "params['temperature'] = 0\n",
    "\n",
    "if 'llama3' in params['model']:\n",
    "    llm = Ollama(model=params['model'], num_ctx = 4096,num_predict=-1, num_gpu=1, temperature=params['temperature'],\n",
    "                    stop=[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\", \"<|reserved_special_token\"])\n",
    "elif 'llama2' in params['model']:\n",
    "    llm = Ollama(model=params['model'], num_ctx = 4096, num_predict=-1, num_gpu=1, temperature=params['temperature'])   # ,num_ctx=2048\n",
    "else:\n",
    "    llm = ChatOpenAI(temperature=0, model_name=params['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts  \n",
    "Setup your prompt templates:\n",
    "- System prompt\n",
    "- \"Map\" prompt for summarising individual articles\n",
    "- \"Reduce\" prompt to combine all summarised information into an appropriate output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in params['model']:\n",
    "    # System Prompt\n",
    "    params['system'] = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are an experienced investment analyst who writes for the Chief Investment Office at a wealth management firm.<|eot_id|>\"\n",
    "    # User Prompt\n",
    "    params['map_template'] = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>The following text delimited in triple quotes is an article:\n",
    "\"\"\"{docs}\"\"\"\n",
    "Based on this article, please identify all key news and calls to action.\n",
    "Helpful Answer:<|eot_id|>'''\n",
    "else:\n",
    "    # System Prompt\n",
    "    params['system'] = \"You are an experienced investment analyst who writes for the Chief Investment Office at a wealth management firm.\"\n",
    "    # User Prompt\n",
    "    params['map_template'] = '''The following text delimited in triple quotes is an article:\n",
    "\"\"\"{docs}\"\"\"   \n",
    "Based on this article, please identify all key news and calls to action.\n",
    "Helpful Answer:'''\n",
    "map_prompt = ChatPromptTemplate.from_messages([(\"system\", params['system']),(\"user\", params['map_template'])])\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Re-write all summaries using the following structure. Do not leave out any information.\n",
    "if 'llama' in params['model']:\n",
    "    params['reduce_template'] = '''<|begin_of_text|><|start_header_id|>user<|end_header_id|>The following is a set of seven article summaries:\n",
    "\"\"\"{doc_summaries}\"\"\"\n",
    "Rewrite all seven article summaries using the following structure. Your entire response should be 2000 words long.\n",
    "Key News: [in prose form]\n",
    "Actionable Insights: [in point form]<|eot_id|>'''\n",
    "else:\n",
    "    params['reduce_template'] ='''The following is a set of seven article summaries:\n",
    "\"\"\"{doc_summaries}\"\"\"\n",
    "Rewrite all seven article summaries using the following structure. Your entire response should be 2000 words long.\n",
    "Key News: [in prose form]\n",
    "Actionable Insights: [in point form]'''\n",
    "reduce_prompt = ChatPromptTemplate.from_messages([(\"system\", params['system']),(\"user\", params['reduce_template'])])\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "# Load docs\n",
    "for f in params['doc_names']:\n",
    "    loader = TextLoader(f,encoding='UTF-8')\n",
    "    docs.append(loader.load()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "Summarise each document individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_results = []\n",
    "\n",
    "start = time.time()\n",
    "for doc_text in docs:\n",
    "    output = map_chain.invoke(doc_text)\n",
    "    map_results.append(output['text'])\n",
    "    print(output['text'])\n",
    "end = time.time()\n",
    "\n",
    "params['map_duration'] = end - start\n",
    "\n",
    "params['intermediate_outputs'] = map_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all summaries into a single string `mapped_docs`.  \n",
    "Each article summary is pre-fixed by the article title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of all articles\n",
    "titles = [f[11:-4] for f in os.listdir(doc_folder) if os.path.isfile(os.path.join(doc_folder,f))]\n",
    "\n",
    "mapped_docs = ''\n",
    "i = 0\n",
    "for doc in params['intermediate_outputs']:\n",
    "    mapped_docs = mapped_docs+ \"Article \"+str(i+1)+\": \"+titles[i] + \"\\n\" + doc + \"\\n\\n\"\n",
    "    i = i + 1\n",
    "params['intermediate_outputs'] = mapped_docs\n",
    "print(mapped_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce  \n",
    "Combine the summaries of all articles into a single \"weekly digest\" with an output format as demanded by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "params['summary'] = reduce_chain.invoke(mapped_docs)['text']\n",
    "end = time.time()\n",
    "\n",
    "params['reduce_duration'] = end - start\n",
    "print(params['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Log File  \n",
    "Write all input hyperparameters and LLM outputs into a log file saved in the home directory as `./zeroshot_summarise_ubs_week_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "params['experiment_date'] = now.strftime(\"%Y-%m-%d\")\n",
    "params['experiment_time'] = now.strftime(\"%H:%M:%S\")\n",
    "try:  \n",
    "    all_params_df = pd.read_csv(log_file)\n",
    "except:\n",
    "    all_params_df = pd.DataFrame()\n",
    "\n",
    "params_df = pd.DataFrame([params])\n",
    "all_params_df = pd.concat([all_params_df,params_df])\n",
    "\n",
    "all_params_df.to_csv(log_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
